{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data collection - Social Web 2022 group 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook displays the code used to create the dataset(s) utilized for the Social Web (2022) final paper for group 26 (A. Anthony Joseph; M.B. Trans; Y. Fan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code utilized here is independent of third party libraries, and interacts directly with Twitter's API endpoints. This was a deliberate design-choice due to better control over data output formatting, as well as the process of learning how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "os.environ['TOKEN'] = '***REMOVED***'\n",
    "\n",
    "\n",
    "def auth():\n",
    "    \"\"\"Returns the API access-token as saved within the environment\"\"\"\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    \"\"\"Prepares the required security-headers expected by Twitter's API-endpoints\"\"\"\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "\n",
    "def create_count_url_v2(keyword, start_date, end_date):\n",
    "    \"\"\"Creates the URL and query-parameters for the count-endpoint based on the parameters defined\n",
    "    further below, allowing us a rough overview of how many tweets we can expect for a given query\"\"\"\n",
    "    search_url = \"https://api.twitter.com/2/tweets/counts/all\"  # Change to the endpoint you want to collect data from\n",
    "\n",
    "    # we should params based on the endpoint you are using\n",
    "    query_params = {'query': keyword, 'start_time': start_date, 'end_time': end_date, 'granularity': 'day'}\n",
    "    return search_url, query_params\n",
    "\n",
    "\n",
    "def create_search_url_v2(keyword, start_date, end_date, max_results=10):\n",
    "    \"\"\"Creates the URL and query-parameters for the search-endpoint based on the parameters defined further below\"\"\"\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"  # Change to the endpoint you want to collect data from\n",
    "\n",
    "    # we should params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,'\n",
    "                      'in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id,edit_history_tweet_ids',\n",
    "        'media.fields': 'duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics,alt_text,variants',\n",
    "        'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "        'poll.fields': 'duration_minutes,end_datetime,id,options,voting_status',\n",
    "        'tweet.fields': 'attachments,author_id,conversation_id,created_at,entities,geo,id,'\n",
    "                        'in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,'\n",
    "                        'source,text,withheld,edit_controls',\n",
    "        'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,'\n",
    "                       'public_metrics,url,username,verified,withheld',\n",
    "                        'next_token': None}\n",
    "    return search_url, query_params\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token=None):\n",
    "    \"\"\"Connects directly to the endpoint specified through the supplied URL, headers and parameters and can pass next_tokens where required\"\"\"\n",
    "    params['next_token'] = next_token  # params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the functions ready for use we can define our query with the variables defined below, and generate the required URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inputs for the request\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"#COP27 -is:retweet\"  # We decided to remove retweets as they make up around 85% of our total query, and adds little to our graphs.\n",
    "start_time = \"2022-11-06T00:00:00.000Z\"  # The start date of the COP27\n",
    "end_time = \"2022-11-20T00:00:00.000Z\"  # The end date of the COP27 after the extension of another day\n",
    "max_results = 500  # The maximum allowed tweets per request\n",
    "\n",
    "url_search = create_search_url_v2(keyword, start_time, end_time, max_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Making sure the requests are working as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_response = connect_to_endpoint(url_search[0], headers, url_search[1])\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Retrieve an estimate of the number of tweets that matches our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url_count = create_count_url_v2(keyword, start_time, end_time)\n",
    "count_response = connect_to_endpoint(url_count[0], headers, url_count[1])\n",
    "print(count_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define empty placeholders and default values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN UNLESS STARTING NEW COLLECTION\n",
    "\n",
    "tweets_downloaded = 0\n",
    "requests_made = 0\n",
    "\n",
    "data = []\n",
    "includes = []\n",
    "metas = []\n",
    "errors = []\n",
    "\n",
    "max_requests = (count_response['meta']['total_tweet_count']/500)*1.1  # we add another 10% as a buffer in case the estimate is wrong.\n",
    "\n",
    "json_response = None\n",
    "next_token = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We start getting the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "while requests_made < max_requests:\n",
    "    try:\n",
    "        if requests_made > 0:  # make sure the first requests has an empty next_token\n",
    "            if \"next_token\" in json_response['meta'].keys():  # the last batch of tweets in a query does not return a next_token\n",
    "                next_token = json_response['meta']['next_token']  # Takes the next_token from the previous output as the following requests' input\n",
    "            else:\n",
    "                print(\"All available tweets retrieved!\")\n",
    "                break\n",
    "        # request a set of up to 500 tweets:\n",
    "        json_response = connect_to_endpoint(url_search[0], headers, url_search[1], next_token)\n",
    "        # store the tweets in our local data-structure that mimicks the structure of the response\n",
    "        for tweet in json_response['data']:\n",
    "            data.append(tweet)\n",
    "        for include in json_response['includes']:\n",
    "            includes.append(include)\n",
    "        if hasattr(json_response, 'errors'):\n",
    "            for error in json_response['errors']:\n",
    "                includes.append(error)\n",
    "        metas.append(json_response['meta'])\n",
    "\n",
    "        # count the amounts of tweets retrieved and requests made, and print the stats:\n",
    "        tweets_downloaded += len(json_response['data'])\n",
    "        requests_made += 1\n",
    "        print('next_token:', next_token, \"Tweets downloaded:\", tweets_downloaded)\n",
    "        print('Meta content:', json_response['meta'])\n",
    "    except ConnectionError as err:  # The API were very unstable as we were downloading the tweets:\n",
    "        print(err, \"... waiting 2 seconds to retry...\")\n",
    "        sleep(2)\n",
    "        continue\n",
    "print(\"The expected amount of requests have been carried out. Check the last response for a next_token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can run the cell below to confirm the amounts of requests made and amount of tweets gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(requests_made)\n",
    "print(tweets_downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Put the returned tweets into a JSON-compatible dictionary and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_file = {\"data\": data,\n",
    "             \"includes\": includes,\n",
    "             \"errors\": errors}\n",
    "\n",
    "with open(\"tweets_hashtag_COP27_exclRetweets_2022-11-06_20.json\", \"w\") as outfile:  # the filename reflects the query and time-frame\n",
    "    json.dump(json_file, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}