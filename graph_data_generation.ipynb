{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from random import random\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('tweets_hashtag_COP27_exclRetweets_2022-11-06_20.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(len(data['data']))\n",
    "\n",
    "# record all the tweets id in the dataset\n",
    "tweet_id_list = []\n",
    "for tweet in data['data']:\n",
    "    tweet_id_list.append(tweet['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nodes(data):\n",
    "    nodes = []\n",
    "    for tweet in data['data']:\n",
    "        # encode new tweets authors to be the nodes, given a bunch of attributes in the public_metrics\n",
    "        if tweet['author_id'] not in [node[0] for node in nodes]:\n",
    "            nodes.append((tweet['author_id'], tweet['public_metrics']))\n",
    "            continue\n",
    "        \n",
    "        # if the author already exists in the nodes list\n",
    "        for i, d in enumerate(nodes):\n",
    "            if tweet['author_id'] == nodes[i][0]:\n",
    "                # sum the values of public_metrics and update the author's attributes in the nodes list\n",
    "                nodes[i] = (tweet['author_id'], \n",
    "                            {key: tweet['public_metrics'].get(key) + nodes[i][1].get(key) \\\n",
    "                            for key in set(tweet['public_metrics']) | set(nodes[i][1])})\n",
    "                break\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(data):\n",
    "    nodes = []\n",
    "    for tweet in data['data']:\n",
    "        # encode new tweets authors to be the nodes, given a bunch of attributes in the public_metrics\n",
    "        if tweet['author_id'] not in [node[0] for node in nodes]:\n",
    "            nodes.append((tweet['author_id'], tweet['public_metrics']))\n",
    "            continue\n",
    "        \n",
    "        # if the author already exists in the nodes list\n",
    "        for i, d in enumerate(nodes):\n",
    "            if tweet['author_id'] == nodes[i][0]:\n",
    "                # sum the values of public_metrics and update the author's attributes in the nodes list\n",
    "                nodes[i] = (tweet['author_id'], \n",
    "                            {key: tweet['public_metrics'].get(key) + nodes[i][1].get(key) \\\n",
    "                            for key in set(tweet['public_metrics']) | set(nodes[i][1])})\n",
    "                break\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_id(data):\n",
    "    tweets_id = {}\n",
    "    # construct a dictionary which uses author_id to look up tweets\n",
    "    for tweet in data['data']:\n",
    "        if tweet['author_id'] not in tweets_id.keys():\n",
    "            tweets_id[tweet['author_id']] = [tweet['id']]\n",
    "        else:\n",
    "            tweets_id[tweet['author_id']].append(tweet['id']) \n",
    "    return tweets_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(data):\n",
    "    hashtags = {}\n",
    "    # construct a dictionary which uses author_id to look up hashtags\n",
    "    for tweet in data['data']:\n",
    "        if 'entities' in tweet:\n",
    "            if 'hashtags' in tweet['entities']:\n",
    "                hashtag_list = tweet['entities']['hashtags']\n",
    "                for hashtag in hashtag_list:\n",
    "\n",
    "                    if tweet['author_id'] not in hashtags.keys():\n",
    "                        hashtags[tweet['author_id']] = [hashtag['tag']]\n",
    "                    else:\n",
    "                        hashtags[tweet['author_id']].append(hashtag['tag']) \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(data):\n",
    "    urls = {}\n",
    "    # construct a dictionary which uses author_id to look up urls\n",
    "    for tweet in data['data']:\n",
    "        if 'entities' in tweet:\n",
    "            if 'urls' in tweet['entities']:\n",
    "                url_list = tweet['entities']['urls']\n",
    "                for url in url_list:\n",
    "\n",
    "                    if tweet['author_id'] not in urls.keys():\n",
    "                        urls[tweet['author_id']] = [url['url']]\n",
    "                    else:\n",
    "                        urls[tweet['author_id']].append(url['url']) \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(data, existing_authors):\n",
    "    links = []\n",
    "    # generate edges based on users interactions (mentions)\n",
    "    for tweet in data['data']:\n",
    "        if 'entities' in tweet:\n",
    "            if 'mentions' in tweet['entities']:\n",
    "                mention_list = tweet['entities']['mentions']\n",
    "                for mention in mention_list:\n",
    "                    if mention['id'] in existing_authors and mention['id'] != tweet['author_id']:\n",
    "                        # the weight for all edges by default is 1\n",
    "                        if (tweet['author_id'], mention['id']) not in [(link[0], link[1]) for link in links]:\n",
    "                            links.append((tweet['author_id'], mention['id'], {'weight': 1}))  \n",
    "                        else:\n",
    "                            # in case there are multiple identical edges, the weight of the edge increase by 1 at a time\n",
    "                            for i , d in enumerate(links):\n",
    "                                if tweet['author_id'] == links[i][0] and mention['id'] == links[i][1]:\n",
    "                                    links[i] = (tweet['author_id'], mention['id'], {'weight': links[i][2].get('weight') + 1})\n",
    "                                    break\n",
    "    return links   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract useful information from the original dataset\n",
    "nodes = get_nodes(data)\n",
    "tweets_id = get_tweets_id(data)\n",
    "hashtags = get_hashtags(data)\n",
    "urls = get_urls(data)\n",
    "links = get_links(data, tweets_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the directed graph \n",
    "dG = nx.DiGraph()\n",
    "dG.add_nodes_from(nodes)\n",
    "dG.add_edges_from(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all isolated nodes\n",
    "dG.remove_nodes_from(list(nx.isolates(dG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(dG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(dG.degree(dG.nodes()))\n",
    "nx.set_node_attributes(dG, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(dG)\n",
    "nx.set_node_attributes(dG, closeness_centrality, 'closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality_numpy(dG)\n",
    "nx.set_node_attributes(dG, eigenvector_centrality, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_centrality = nx.betweenness_centrality(dG)\n",
    "nx.set_node_attributes(dG, between_centrality, 'betweenness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk:\n",
    "# with open('nodes_with_centralities.pkl', 'wb') as f:\n",
    "#     pickle.dump(dG.nodes(data=True), f)\n",
    "# with open('nodes.pkl', 'wb') as f:\n",
    "#     pickle.dump(nodes, f)\n",
    "# with open('tweet_id_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(tweet_id_list, f)\n",
    "# with open('hashtags.pkl', 'wb') as f:\n",
    "#     pickle.dump(hashtags, f)\n",
    "# with open('urls.pkl', 'wb') as f:\n",
    "#     pickle.dump(urls, f)\n",
    "# with open('links.pkl', 'wb') as f:\n",
    "#     pickle.dump(dG.edges(data=True), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08c0b4edf41f4e68a97cc6457870175c3163ec44eea4baa73984104126b7b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
