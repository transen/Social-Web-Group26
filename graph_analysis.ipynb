{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from random import random\n",
    "import json\n",
    "import pickle\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load from disk \n",
    "with open('nodes_with_centralities.pkl', 'rb') as f:\n",
    "    nodes = pickle.load(f)\n",
    "# a look-up dictionary: tweets_id[user_id] = [IDs for all tweets from this user]\n",
    "with open('tweets_id.pkl', 'rb') as f:\n",
    "    tweets_id = pickle.load(f)\n",
    "# a look-up dictionary: hashtags[user_id] = [all hashtags used by this user]\n",
    "# repetitive hashtags are saved because the frequency or percentage for each hashtag might be of interest\n",
    "with open('hashtags.pkl', 'rb') as f:\n",
    "    hashtags = pickle.load(f)\n",
    "# a look-up dictionary: urls[user_id] = [all urls linked by this user]\n",
    "with open('urls.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "\n",
    "with open('links.pkl', 'rb') as f:\n",
    "    links = pickle.load(f)\n",
    "\n",
    "with open('tweets_hashtag_COP27_exclRetweets_2022-11-06_20.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the graph\n",
    "dG = nx.DiGraph()\n",
    "dG.add_nodes_from(nodes)\n",
    "dG.add_edges_from(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get top influensers in different measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# possible metrics: engagement, degree, closeness, eigenvector, betweenness\n",
    "def get_top5_influensers(metric):\n",
    "    hashtag_dict = {}\n",
    "    url_dict = {}\n",
    "    if metric == 'engagement':\n",
    "        top5_influensers = sorted(dG.nodes(), key = lambda x: dG.nodes[x]['reply_count'] + dG.nodes[x]['quote_count'] + dG.nodes[x]['retweet_count'] + dG.nodes[x]['like_count'])[-1:-6:-1]\n",
    "    else:\n",
    "        top5_influensers = sorted(dG.nodes(), key = lambda x: dG.nodes[x][metric])[-1:-6:-1]\n",
    "\n",
    "    for user in top5_influensers:\n",
    "        try:\n",
    "            hashtag_dict[user] = Counter(list(map(lambda x: x.lower(), hashtags[user]))).most_common()\n",
    "            url_dict[user] = Counter(urls[user]).most_common()\n",
    "        except:\n",
    "            pass\n",
    "    return (top5_influensers, hashtag_dict, url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_engagement_count(user_list):\n",
    "    # return the engagement counts for a list of users\n",
    "    engagement_count = []\n",
    "    for user_id in user_list:\n",
    "        engagement_count.append(dG.nodes[user_id]['reply_count'] + dG.nodes[user_id]['quote_count'] + dG.nodes[user_id]['retweet_count'] + dG.nodes[user_id]['like_count'])\n",
    "    return engagement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the result of get_top5_influensers(metric) is in the format of (user_ids, hashtag_counter, url_counter)\n",
    "top5 = get_top5_influensers('betweenness')\n",
    "print(top5[0]) # print the user IDs for the top 5 users under a certain metric\n",
    "print(get_engagement_count(top5[0])) # print the engagement counts for the top 5 users under a certain metric\n",
    "# print(top5[1])\n",
    "# print(top5[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### visualizing top users hashtags / urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_hashtag_dist(user_rank):\n",
    "    # convert the hashtag distribution of a user into a dataframe\n",
    "    users_hashtags = top5[1]\n",
    "    hashtag_df = pd.DataFrame(users_hashtags[top5[0][user_rank]], columns=['hashtags', 'frequency'])\n",
    "    return hashtag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_url_dist(user_rank):\n",
    "    # convert the url distribution of a user into a dataframe\n",
    "    users_urls = top5[2]\n",
    "    hashtag_df = pd.DataFrame(users_urls[top5[0][user_rank]], columns=['urls', 'frequency'])\n",
    "    return hashtag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select which user's hashtag distribution you want to visualize by filling in the user's rank number -> 0 = top one\n",
    "df_to_visulize = get_user_hashtag_dist(0) \n",
    "ax = df_to_visulize.plot(kind='bar', x='hashtags', figsize=(15, 5), legend=False)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select which user's url distribution you want to visualize by filling in the user's rank number -> 0 = top one\n",
    "df_to_visulize = get_user_url_dist(1)\n",
    "ax = df_to_visulize.plot(kind='bar', x='urls', figsize=(15, 5), legend=False)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('URLs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The user who tweets the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# top 5 frequent users\n",
    "sorted(tweets_id, key = lambda x: len(tweets_id[x]))[-1:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# number of tweets a certain user tweets\n",
    "len(tweets_id['1252764865'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print the user's tweets if needed\n",
    "for tweet in data['data']:\n",
    "    if tweet['author_id'] == '1252764865':\n",
    "        # print(tweet)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How many distinct hashtags there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hashtag_counter = Counter()\n",
    "for node in nodes:\n",
    "    try:\n",
    "        hashtag_counter += Counter(list(map(lambda x: x.lower(), hashtags[node[0]])))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(hashtag_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How many times each of the hashtag has been posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# in the format of [(hashtag, frequency)]\n",
    "hashtag_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Total Amount of hashtags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hashtags_sum = sum([frequency for (hashtag, frequency) in hashtag_counter.most_common()])\n",
    "hashtags_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Top 30 Hashtags among all users (excluding #cop27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top30_hashtags = hashtag_counter.most_common()[1:31]\n",
    "print(hashtag_counter.most_common()[1:11])\n",
    "hashtag_df = pd.DataFrame(top30_hashtags, columns=['hashtags', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = hashtag_df.plot(kind='bar', x='hashtags', figsize=(15, 5), legend=False)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# phrases contain non-alphabetic symbols \n",
    "[hashtag for (hashtag, frequency) in top30_hashtags if not hashtag.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get hashtags per user-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = '73882819'\n",
    "Counter(list(map(lambda x: x.lower(), hashtags[user_id]))).most_common()[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Temporal tweets distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temporal_list= []\n",
    "for tweet in data['data']:\n",
    "    temporal_list.append((tweet['id'], datetime.strptime(tweet['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\").date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temporal_df = pd.DataFrame(temporal_list, columns=['tweet', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = temporal_df.groupby(['date']).size().plot(kind='bar', x='date', figsize=(15, 5), legend=False)\n",
    "ax2.set_ylabel('Amount of tweets')\n",
    "ax2.set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_md')\n",
    "# nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def convert_sentiment(data):\n",
    "#     tweet_sentiments = [0, 0, 0]\n",
    "#     for polarity in data:\n",
    "#         if polarity < 0:\n",
    "#             tweet_sentiments[0] += 1\n",
    "#         elif polarity == 0:\n",
    "#             tweet_sentiments[1] += 1\n",
    "#         else:\n",
    "#             tweet_sentiments[2] += 1\n",
    "#     return tweet_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sentiments = []\n",
    "# subjectivity = []\n",
    "# for tweet in data['data']:\n",
    "#     if tweet['id'] in tweets_id['1498985846078377985']:\n",
    "#         doc = nlp(tweet['text'])\n",
    "#         sentiments.append(doc._.blob.polarity)\n",
    "#         subjectivity.append(doc._.blob.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # The average subjectivity for a certain user, the higher the value, the more subjective the user's speech \n",
    "# mean(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# y = convert_sentiment(sentiments)\n",
    "\n",
    "# mylabels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "# myexplode = [0, 0, 0]\n",
    "# plt.pie(y, explode = myexplode)\n",
    "# total = sum(y)\n",
    "# plt.legend(\n",
    "#     loc='lower right',\n",
    "#     labels=['%s, %1.2f%%' % (l, (float(s) / total) * 100) for l, s in zip(mylabels, y)],\n",
    "#     prop={'size': 11},\n",
    "#     bbox_to_anchor=(1.1, 0.7),\n",
    "#     bbox_transform=plt.gcf().transFigure\n",
    "# )\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### New implementation (API mentioned in the lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    url = 'http://text-processing.com/api/sentiment/'\n",
    "    params = {'text': text}\n",
    "\n",
    "    resp = requests.post(url, params)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentiments2 = []\n",
    "counter = 0\n",
    "for tweet in data['data']:\n",
    "    # set the maximum number of tweets to be analyzed to 100 to avoid being blocked by the API\n",
    "    if counter == 100:\n",
    "        break\n",
    "    # pass in the user id you want to do sentiment analysis\n",
    "    if tweet['id'] in tweets_id['1498985846078377985']:\n",
    "        label = get_sentiment(tweet['text'])\n",
    "        sentiments2.append(label)\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sentiment2(data):\n",
    "    # concvert the sentiments into frequency counts\n",
    "    tweet_sentiments = [0, 0, 0]\n",
    "    for label in data:\n",
    "        if label == 'neg':\n",
    "            tweet_sentiments[0] += 1\n",
    "        elif label == 'neutral':\n",
    "            tweet_sentiments[1] += 1\n",
    "        else:\n",
    "            tweet_sentiments[2] += 1\n",
    "    return tweet_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the sentiment analysis for a certain user using pie chart\n",
    "y = convert_sentiment2(sentiments2)\n",
    "mylabels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "myexplode = [0, 0, 0]\n",
    "plt.pie(y, explode = myexplode)\n",
    "total = sum(y)\n",
    "plt.legend(\n",
    "    loc='lower right',\n",
    "    labels=['%s, %1.2f%%' % (l, (float(s) / total) * 100) for l, s in zip(mylabels, y)],\n",
    "    prop={'size': 11},\n",
    "    bbox_to_anchor=(1.1, 0.7),\n",
    "    bbox_transform=plt.gcf().transFigure\n",
    ")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Central hub visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the most central node under a certain metric\n",
    "top_influenser = get_top5_influensers('closeness')[0][0]\n",
    "dG.nodes[top_influenser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the descendants of the central node (top_influenser) at a certain distance\n",
    "reachable_nodes = nx.descendants_at_distance(dG, top_influenser, 1)\n",
    "# the descendants_at_distance() function be default exludes the source node itself, manually add the source node\n",
    "reachable_nodes.add(top_influenser)\n",
    "# construct a subgraph of all the reachable nodes\n",
    "central_hub = dG.subgraph(reachable_nodes).copy()\n",
    "# remove isolated nodes\n",
    "central_hub.remove_nodes_from(list(nx.isolates(central_hub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nx.info(central_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_graph_node_centrality(graph, centrality_measure, label_offset=0.08, plot_margin=0.1, show_edge_attribute=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    pos = nx.spring_layout(graph)\n",
    "\n",
    "    if centrality_measure not in ['betweenness', 'eigenvector', 'degree', 'closeness', 'engagement']:\n",
    "        raise ValueError\n",
    "    # set nodes' color theme based on the centrality measure taken\n",
    "    if centrality_measure == 'engagement':\n",
    "        node_colors = [d['reply_count'] + d['quote_count'] + d['retweet_count'] + d['like_count'] for (n, d) in graph.nodes(data = True)]\n",
    "    else:\n",
    "        node_colors = [d[centrality_measure] for (n, d) in graph.nodes(data = True)]\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=30, node_color=node_colors)\n",
    "    # uncomment the below 2 lines to enable dynamic node size\n",
    "    # node_size = [d['retweet_count'] for (n, d) in graph.nodes(data = True)]\n",
    "    # nx.draw_networkx_nodes(graph, pos, node_size=node_size, node_color=node_colors)\n",
    "    \n",
    "    nx.draw_networkx_edges(graph, pos, edgelist = graph.edges())\n",
    "\n",
    "    if show_edge_attribute:\n",
    "        edge_labels = {(u, v): d['weight'] for (u, v, d) in graph.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels)\n",
    "        \n",
    "    xs = [p[0] for p in pos.values()] # extract all x...\n",
    "    ys = [p[1] for p in pos.values()] # ...and y values from edges positions\n",
    "    ax.set_xlim((min(xs) - plot_margin, max(xs) + plot_margin))\n",
    "    ax.set_ylim((min(ys) - plot_margin, max(ys) + plot_margin))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# possible metric to set the color theme of the nodes: 'betweenness', 'eigenvector', 'degree', 'closeness', 'engagement'\n",
    "show_graph_node_centrality(central_hub, 'closeness')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08c0b4edf41f4e68a97cc6457870175c3163ec44eea4baa73984104126b7b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}